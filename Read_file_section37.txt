POD ALLOCATION

NOte: HA SETUP for Handons

****What is Scheduling****

1) Scheduling is a process to assign the Pods to Nodes, so that Kubectl can run them.
2) Scheduler : It’s a component on Kubernetes Master Node,which decide the Pods assignment on nodes.


****Scheduling Process****

1) Kubernetes Scheduler Select the suitable node for Pods -
2) Resource Request vs Available Node Resources
3) Configuration like Node Labels
4) nodeSelector, Affinity and Anti-Affinity

****nodeSelector****

1) nodeSelector is define in Pod Spec to limit which Node(s) the pod can be scheduled on.
2) NodeSelector use Labels to select the suitable Node.

example of manifest/definition/specification

apiVersion: v1
kind: Pod
metadata:
 name: cassandra
spec:
 containers:
 - name: cassandra
 image: cassandra
 nodeSelector:
   disktype: ssd



****nodeName****

1) User can bypass scheduling and assign Pod to a Specific Node using Node Name.
2) NodeName is typically is not good option to use for Pod scheduling due to its limitations. 

example of manifest/definition/specification

apiVersion: v1
kind: Pod
metadata:
 name: nginx
spec:
 containers:
 - name: nginx
 image: nginx
 nodeName: kubeNode-01

 ***HANDS ON ****

# Execute on HA SETUP Master node
# Section 37 Lab Number 236

# steps
# 1) Go to directory where scripts are stored, cd <diretory_name>
# 2) Go to vi editor and name it using nodeselector.yaml
# 3) copy the only first content up to first _ _ _ (nodeselector) from above and save the file.
# 4) create a pod, kubectl apply -f <file-name.yml>
# 5) check if pod is created, kubectl get pods (Note: pods is in pending state because need to configured label on nodes)
# 6) check describe of pod, kubectl desribe pod <pod-name>
# 7) To Check label is configured on nodes "Kubectl get nodes --show-labels"
# 8) Create a lable on nodes
#   kubectl label nodes <node_name> key=value
#   kubectl label nodes worker-node1 disktype=ssd
# Note: We get node name from "kubectl get nodes"
# 9) Check if pod is created, kubectl get pods
# 10) Let deploy pod using nodeName
# 11) Go to vi editor and name it using nodename.yaml
# 12) copy the only second contents up to  _ _ _ (nodename) from above and save the file.
# 13) Change the nodeName parameters has nodename got in "kubectl get nodes"
#testingworld@k8s-master-IP36:~/safscripts$ kubectl get nodes
#NAME                             STATUS   ROLES           AGE   VERSION
#k8s-master-ip36.blr.local        Ready    control-plane   32d   v1.30.3
#k8s-workernode1-ip37.blr.local   Ready    <none>          32d   v1.30.3
#k8s-workernode2-ip38.blr.local   Ready    <none>          32d   v1.30.3

#apiVersion: v1
#kind: Pod
#metadata:
  #name: nginx-nodename
#spec:
  #containers:
   # - name: nginx
   #   image: nginx
  #nodeName: k8s-workernode1-ip37.blr.loca

# 14) create a pod, kubectl apply -f <file-name.yml>
# 15) check if pod is created, kubectl get pods
# 16) check describe of pod, kubectl desribe pod <pod-name>
# 18) Let deploy pod using resource request and nodeSelector
# 19) Go to vi editor and name it using nodename.yaml
# 20) Copy the only Thrid contents up to  _ _ _ (nodeSelector) from above and save the file.
#apiVersion: v1
#kind: Pod
#metadata:
  #name: frontend-app
#spec:
  #containers:
    #- name: app
    #  image: alpine
    #  command:
    #    - sleep
    #    - '3600'
    #  resources:
    #    requests:
    #      memory: 64Mi
    #      cpu: 1000m
  #nodeSelector:
    #disktype: ssd
# 21) create a pod, kubectl apply -f <file-name.yml>
# Note: POD will be deployed only if resource request is met then label "disktype: ssd" is met
# if both the request met, pod will get deployed.
# if any one is not metting the request, then pod will created but it will on pending state.
# 22) check if pod is created, kubectl get pods
# 23) check describe of pod, kubectl desribe pod <pod-name>
# 24) vi editor with name req2-pod.yml
# 25) Copy the only fourth contents where we have not specified the nodeselector from above and save the file.
#apiVersion: v1
#kind: Pod
#metadata:
  #name: frontend-app-2
#spec:
  #containers:
    #- name: app
    #  image: alpine
    #  command:
    #    - sleep
    #    - '3600'
    #  resources:
    #    requests:
    #      memory: 64Mi
    #      cpu: 1000m
# 26) create a pod, kubectl apply -f <file-name.yml>
# Note: if resource is available on any of nodes, POD will be deployed on that node
# 27) Check if pod is created, kubectl get pods
# 28) Check describe of pod, kubectl desribe pod <pod-name>

==================

****DAEMONSETS****

1) Automatically Run a copy of a Pod on Each Node.
2) DaemonSet run a Copy of a Pod on New Node as they added to Cluster.
3) DaemonSet will be helpful in case of Monitoring, Log Collection, Proxy Configuration etc.


****Scheduling & DaemonSets****

1) DaemonSets follows normal scheduling Rules around node labels, taints and tolerations.
2) If pods normally not scheduled on a Node, daemonset will also not create copy of Pod on that node.


***HANDS ON ***

# Execute on HA SETUP Master node
# Section 37 Lab Number 238

# Manifest explanation:

# kind: DaemonSet -> is a object.
# Selector can be used to identify the pods running in your K8s on the basis of that particular label.
# Selector has there own template, metadat and specification.
# Note: Labels under selector and template should be same.


# steps
# 1) Go to directory where scripts are stored, cd <diretory_name>
# 2) Go to vi editor and name it using daemonsets.yaml
# 3) Copy the content from above and save the file.
# 4) Create a pod, kubectl apply -f <file-name.yml>
# 5) Check if pod is created, kubectl get daemonsets
# we notice that 3 daemonsets are running, one each on all nodes.
# suppose if u have foure nodes, then it will have 4 daemonsets, one each on all nodes.
# 6) Check pods, kubectl get pods -o wide
# we notice that 3 pods are running, one each on all nodes 
# 7) Check describe of pod, kubectl describe pod <pod-name>
# 8) Check deamonset, kubectl describe daemonsets <daemonset_name>

=======

****Static Pods****

1) Static Pods directly managed by Kubelet on K8s Nodes.
2) K8s API Server is not required for Static Pods.
3) kubelet watches each static Pod (and restarts it if it fails).
4) Kubelet automatically creates Static Pods from YAML file located at manifest path on the Node.

****Mirror Pods****

1)  Kubelet will create the Mirror pod for each Static Pod.
2)  Mirror pods allows user to monitor Static Pods via K8s APIs.
3)  User can’t change or Update Static Pods via Mirror Pods.

***HANDS ON****

1) We can create static pods and how we can get the static pod monitor by mirror pods.
2) Note: Static pod is not managed by master node or the control plane.
3) We have to ssh to the worker node.
4) On Master node execute "kubectl get nodes"
5) On master node, check the pods available "kubectl get pods"
6) GO to any workernode, ssh to worker node using putty.
7) create a manifest file on worker node. why ? we need to create a mainfest file on worker node cos kubelet will
   look for YAML file for the static pods.
8) Go to this path/location to store manifest file of static pod on worker node
   cd /etc/Kubernetes/manifests
9) ls or ll (On workernode)
10) vi static-pod.yml (On workernode)
11) copy these content in file and save and exits
apiVersion: v1
kind: Pod
metadata:
  name: nginx-staticpod
  containers:
    - name: nginx
      image: nginx

Note: Once we save the file, kubelet will automatically read the file and create a pod, we need to wait for sometime.
if you dont want to wait, restart the kubelet.
12) On worker node, "sudo systemctl restart kubelet"
13) Go to master node and execute "kubectl get pods -o wide"
  We notice that pod is created.
  So this is the mirror pod of my static pod, which is running on my worker node.
14) Describe the pod from master node
  kubectl describe pod <pod_name>
15) On master node and execute "kubectl get pods -o wide"
16) Now if we want to delete the pod from master node
   kubectl delete pod <pod-name>
   Note: It says deleted, but when we execute "kubectl get pod" we notice that pod is still running
17) Describe the pod from master node again
  kubectl describe pod <pod_name>
  we notice that no change in states, cos we cannot perform any task/work/chnages from mirror pod to static pod.
  Note: the pod which is on master node is a mirror pod.
18) So when we can use staic pod, whenever you want to put some boot starps on your node, you can execute that 
    particular thing by usinf ststic podd.
19) Note: Static pods is not being managed by the kubernetes control plane or K8s master node.

========

****Node Affinity****

1) Node Affinity is enhanced version of NodeSelector.
2) Node Affinity is used for Pods Allocation on Worker Nodes.
3) Not to Schedule Pod on Nodes is achieve via Node Anti-Affinity.
4) Anti-Affinity is Opposite of Affinity and NodeSelector Concept.

comparing Node Selector example manifest and Node Affinity example manifest

Node Selector example manifest
apiVersion: v1
kind: Pod
metadata:
 name: nginx-nodeselector
spec:
 containers:
 - name: nginx
 image: nginx
 nodeSelector:
    disktype: ssd

 =====

Node Affinity example manifest
...
spec:
 containers:
 - name: nginx
 image: nginx
 affinity:
 nodeAffinity:
 requiredDuringSchedulingIgnoredDuringExecution:
 nodeSelectorTerms:
 - matchExpressions:
 - key: disktype
 operator: In
 values:
 - ssd


Node Affinity
1) requiredDuringSchedulingIgnoredDuringExecution : Must fulfil the condition at the time of Pod Creation. Also Called Hard Affinity.
2) IgnoredDuringExecution - Pod will still run if labels on a node change and affinity rules are no longer met. 
3) preferredDuringSchedulingIgnoredDuringExecution : Prefer Node which will fulfil the condition but will not guarantee. Also Called Soft Affinity.
4) requiredDuringSchedulingRequiredDuringExecution : Not added but will be available in future.



Node Affinity example (see operator)
...
spec:
 containers:
 - name: nginx
 image: nginx
 affinity:
 nodeAffinity:
 requiredDuringSchedulingIgnoredDuringExecution:
 nodeSelectorTerms:
 - matchExpressions:
 - key: disktype
 operator: In
 values:
 - ssd


 Node Anti-Affinity example (see operator)

...
spec:
 containers:
 - name: nginx
 image: nginx
 affinity:
 nodeAffinity:
 requiredDuringSchedulingIgnoredDuringExecution:
 nodeSelectorTerms:
 - matchExpressions:
 - key: disktype
 operator: NotIn
 values:
 - ssd

# NotIn means not to execute pod on node which has  label disktype=ssd.

 **** HANDS ON ****

# Execute on HA SETUP Master node
# Section 37 Lab Number 242


# steps
# 1) Go to directory where scripts are stored, cd <diretory_name>
# 2) Go to vi editor and name it using nodeaffinity.yaml
# 3) Copy the first content from above and save the file.
#apiVersion: v1
#kind: Pod
#metadata:
#  name: nginx-nodeaffinity
#spec:
#  containers:
#    - name: nginx
#      image: nginx
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: disktype
#                operator: In
#                values:
#                  - ssd

# 4) execute folowing command to check label attch to node
#    kubectl get nodes --show-labels
# 5) Create a pod, kubectl apply -f <file-name.yml>
# 6) Check if pod is created, "kubectl get pods -o  wide"
# we see that pod is deployed on node which has label disktype=ssd
# 7) Now create one more manifest for node anti-affinity
# 8) Go to vi editor and name it using node-anti-affinity.yaml
# 9) Copy the first second content from above and save the file.
#apiVersion: v1
#kind: Pod
#metadata:
#  name: nginx-nodeaffinity
#spec:
#  containers:
#    - name: nginx
#      image: nginx
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: disktype
#                operator: NotIn
#                values:
#                  - ssd
# 10) Create a pod, kubectl apply -f <file-name.yml>
# 11) Check if pod is created, "kubectl get pods -o  wide"
#Note: we see that pod is deployed on node which does not have this label disktype=ssd

=====


